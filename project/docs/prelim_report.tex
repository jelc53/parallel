\documentclass[12pt,letterpaper,twoside]{article}

\newif\ifsolution\solutiontrue   % Include the solutions
%\newif\ifsolution\solutionfalse  % Exclude the solutions

\usepackage{cme213}
\usepackage{xcolor}
\usepackage{float}
\usepackage{graphicx}

\newcommand{\T}[1]{\text{\texttt{#1}}}
\newcommand{\V}[1]{\text{\textit{#1}}}

\begin{document}

{\centering \textbf{Preliminary Report: Neural Networks on CUDA\\}}
\vspace*{-8pt}\noindent\rule{\linewidth}{1pt}

Goal of the final project is to implement a neural network with parallel 
matrix-matrix operations across four GPUs. Our neural network will be used
to identify digits from hand-written images from the MNIST dataset.

\paragraph{Part 1: Accelerated Matrix Multiplication} Idea: parallelize generalized
inplace matrix mulitplication $C = \alpha*A*B + \beta*C$ across cores of one GPU. We 
will be able to use this to excute multiple steps in the neural network's feed forward 
and back propagation operations.

\begin{figure}[!htbp]
    \centering
    \includegraphics[scale=0.7]{gemm_naive.png}
    \caption{Naive matrix multiplication on GPU. Source: NVIDIA Programming Guide.}
\end{figure}

Our first implementation is correct but naive and inefficient. We ask each thread to 
compute a single value of the output matrix C. This requires each thread to read an 
entire row from matrix A and column from matrix B, resulting in a lot of repeated 
global memory accesses for the same information as we work our way through the 
multiplication. See code sample below.

\begin{cpp}
/* 
Routine to perform an in-place GEMM, i.e., C := alpha*A*B + beta*C
*/
__global__ 
void kernelGEMM(nn_real* __restrict__ A, nn_real* __restrict__ B, 
        nn_real* __restrict__ C, nn_real alpha, nn_real beta, 
            int M, int N, int K) 
{
    // Each thread computes one element of C
    // by accumulating results into Cvalue
    nn_real Cvalue = 0;
    int row = blockIdx.y * blockDim.y + threadIdx.y;
    int col = blockIdx.x * blockDim.x + threadIdx.x;

    if (row < M && col < N) 
    {
        for (int e = 0; e < K; ++e) 
        Cvalue += A[row + M*e] * B[e +  K*col];	
        
    C[row + col*M] = alpha*Cvalue + beta*C[row + col*M];
    }
}
\end{cpp}

\begin{verbatim}
Output from code
----------------

*** Grading mode 4 ***

main -g 4
Number of MPI processes = 1
Number of CUDA devices = 4

Entering GEMM Benchmarking mode! Stand by.

Starting GEMM 1: M = 3200; N = 4000; K = 3136
GEMM matched with reference successfully! Rel diff = 1.00042e-07
Time for reference GEMM implementation: 0.0710125 seconds
Time for my GEMM implementation: 3.04307 seconds
Completed GEMM 1

Starting GEMM 2: M = 3200; N = 400; K = 4000
GEMM matched with reference successfully! Rel diff = 4.57243e-07
Time for reference GEMM implementation: 0.00806695 seconds
Time for my GEMM implementation: 0.389439 seconds
Completed GEMM 2

Starting GEMM 3: M = 3200; N = 40; K = 4000
GEMM matched with reference successfully! Rel diff = 7.29707e-07
Time for reference GEMM implementation: 0.00140079 seconds
Time for my GEMM implementation: 0.0438419 seconds
Completed GEMM 3

*** Tests are complete ***
\end{verbatim}

In subsequent implementations we will try to improve the performance of this 
generalized matrix multiplication algorithm. For example, one speed-up would be
to make use of shared memory. To do this, we could have each thread block compute
a sub-matrix (of size matching thread block dimensions) in the output matrix, with
just the required blocks from A and B read into shared memory. This allows us to 
make use of increased access speed of shared memory and take advantage of a more
coalesced global memory access pattern.


\paragraph{Part 2: Parallelize Neural Network (Single GPU)} Idea: training a neural
network involves repeatedly updating weights and biases at each layer through forward
and backwards propagation. Internally, these operations are mostly various adaptations
of genralized matrix multiplication! We want to take existing sequential code and 
parallelize each using cuda code from part 1.

\begin{itemize}
    \item \textbf{Feed forward.} ...

    \item \textbf{Back propagation.} ...

    \item \textbf{Gradeint descent.} ...
\end{itemize}

\textbf{Remarks on debgugging.} This step required significant debugging effort given 
the inherent complexity of indexing many different variations of parallel matrix 
operations. My approach here was to add \texttt{#if} and \texttt{#endif} statements 
after each major step of the sequential algorithm and try to reproduce that step using 
my parallel code. To test whether the two implementations matched, I re-used existing 
\texttt{checkErrors} and \textt{checkNNErrors} from the provided starter code test suite.


\paragraph{Part 3: Parallelize Training Batches (Multiple GPUs)} Idea: while each epoch 
needs to be executed sequentially, we can perform forwards and backwards propagation on 
batches within each epoch independently (and therefore in parallel). Our code from part 
2 should already make good use of hardware resources on a single GPU, however, in this 
project we have access to multiple GPUs! We use MPI to help coordinate sending different
training batches to different GPUs as well as receiving back and aggregating outputs from
each batch.

...
...

\begin{cpp}
    ...
    ...
\end{cpp}


\paragraph{Part 4: Profiling Parallel Code} Idea: use NVIDIA's Insight and Compute Systems
tool to interrogate performance of code and assess how to improve.

Insert profile screenshots ...
... something about focusing on arithmetic intensity
... also latency bound vs memory bound vs compute bound


Ideas to improve performance:
\begin{itemize}
    \item \textbf{Matrix multiplication.} ...

    \item \textbf{Kernel usage.} ...

    \item \textbf{Batching logic.} ... 

\end{itemize}

\end{document}
